{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Logistic Regression - Cumulative Lab\n\n## Introduction\n\nIn this cumulative lab, you will walk through a complete machine learning workflow with logistic regression, including data preparation, modeling (including hyperparameter tuning), and final model evaluation.\n\n## Objectives\n\nYou will be able to:\n\n* Practice identifying and applying appropriate preprocessing steps\n* Perform an iterative modeling process, starting from a baseline model\n* Practice model validation\n* Practice choosing a final logistic regression model and evaluating its performance"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Your Task: Complete an End-to-End ML Process with Logistic Regression on the Forest Cover Dataset\n\n![forest road](https://curriculum-content.s3.amazonaws.com/data-science/images/forest_road.jpg)\n\n<span>Photo by <a href=\"https://unsplash.com/@von_co?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Ivana Cajina</a> on <a href=\"https://unsplash.com/s/photos/forest-satellite?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></span>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Business and Data Understanding\n\nHere we will be using an adapted version of the forest cover dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/covertype). Each record represents a 30 x 30 meter cell of land within Roosevelt National Forest in northern Colorado, which has been labeled as `Cover_Type` 1 for \"Cottonwood/Willow\" and `Cover_Type` 0 for \"Ponderosa Pine\". (The original dataset contained 7 cover types but we have simplified it.)\n\nThe task is to predict the `Cover_Type` based on the available cartographic variables:"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nimport pandas as pd\n\ndf = pd.read_csv('data/forest_cover.csv')  \ndf",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 5,
     "data": {
      "text/plain": "       Elevation  Aspect  Slope  ...  Soil_Type_38  Soil_Type_39  Cover_Type\n0           2553     235     17  ...             0             0           0\n1           2011     344     17  ...             0             0           0\n2           2022      24     13  ...             0             0           0\n3           2038      50     17  ...             0             0           0\n4           2018     341     27  ...             0             0           0\n...          ...     ...    ...  ...           ...           ...         ...\n38496       2396     153     20  ...             0             0           0\n38497       2391     152     19  ...             0             0           0\n38498       2386     159     17  ...             0             0           0\n38499       2384     170     15  ...             0             0           0\n38500       2383     165     13  ...             0             0           0\n\n[38501 rows x 53 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Elevation</th>\n      <th>Aspect</th>\n      <th>Slope</th>\n      <th>Horizontal_Distance_To_Hydrology</th>\n      <th>Vertical_Distance_To_Hydrology</th>\n      <th>Horizontal_Distance_To_Roadways</th>\n      <th>Hillshade_9am</th>\n      <th>Hillshade_Noon</th>\n      <th>Hillshade_3pm</th>\n      <th>Horizontal_Distance_To_Fire_Points</th>\n      <th>Wilderness_Area_1</th>\n      <th>Wilderness_Area_2</th>\n      <th>Wilderness_Area_3</th>\n      <th>Soil_Type_1</th>\n      <th>Soil_Type_2</th>\n      <th>Soil_Type_3</th>\n      <th>Soil_Type_4</th>\n      <th>Soil_Type_5</th>\n      <th>Soil_Type_6</th>\n      <th>Soil_Type_7</th>\n      <th>Soil_Type_8</th>\n      <th>Soil_Type_9</th>\n      <th>Soil_Type_10</th>\n      <th>Soil_Type_11</th>\n      <th>Soil_Type_12</th>\n      <th>Soil_Type_13</th>\n      <th>Soil_Type_14</th>\n      <th>Soil_Type_15</th>\n      <th>Soil_Type_16</th>\n      <th>Soil_Type_17</th>\n      <th>Soil_Type_18</th>\n      <th>Soil_Type_19</th>\n      <th>Soil_Type_20</th>\n      <th>Soil_Type_21</th>\n      <th>Soil_Type_22</th>\n      <th>Soil_Type_23</th>\n      <th>Soil_Type_24</th>\n      <th>Soil_Type_25</th>\n      <th>Soil_Type_26</th>\n      <th>Soil_Type_27</th>\n      <th>Soil_Type_28</th>\n      <th>Soil_Type_29</th>\n      <th>Soil_Type_30</th>\n      <th>Soil_Type_31</th>\n      <th>Soil_Type_32</th>\n      <th>Soil_Type_33</th>\n      <th>Soil_Type_34</th>\n      <th>Soil_Type_35</th>\n      <th>Soil_Type_36</th>\n      <th>Soil_Type_37</th>\n      <th>Soil_Type_38</th>\n      <th>Soil_Type_39</th>\n      <th>Cover_Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2553</td>\n      <td>235</td>\n      <td>17</td>\n      <td>351</td>\n      <td>95</td>\n      <td>780</td>\n      <td>188</td>\n      <td>253</td>\n      <td>199</td>\n      <td>1410</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2011</td>\n      <td>344</td>\n      <td>17</td>\n      <td>313</td>\n      <td>29</td>\n      <td>404</td>\n      <td>183</td>\n      <td>211</td>\n      <td>164</td>\n      <td>300</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022</td>\n      <td>24</td>\n      <td>13</td>\n      <td>391</td>\n      <td>42</td>\n      <td>509</td>\n      <td>212</td>\n      <td>212</td>\n      <td>134</td>\n      <td>421</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2038</td>\n      <td>50</td>\n      <td>17</td>\n      <td>408</td>\n      <td>71</td>\n      <td>474</td>\n      <td>226</td>\n      <td>200</td>\n      <td>102</td>\n      <td>283</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018</td>\n      <td>341</td>\n      <td>27</td>\n      <td>351</td>\n      <td>34</td>\n      <td>390</td>\n      <td>152</td>\n      <td>188</td>\n      <td>168</td>\n      <td>190</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>38496</th>\n      <td>2396</td>\n      <td>153</td>\n      <td>20</td>\n      <td>85</td>\n      <td>17</td>\n      <td>108</td>\n      <td>240</td>\n      <td>237</td>\n      <td>118</td>\n      <td>837</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>38497</th>\n      <td>2391</td>\n      <td>152</td>\n      <td>19</td>\n      <td>67</td>\n      <td>12</td>\n      <td>95</td>\n      <td>240</td>\n      <td>237</td>\n      <td>119</td>\n      <td>845</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>38498</th>\n      <td>2386</td>\n      <td>159</td>\n      <td>17</td>\n      <td>60</td>\n      <td>7</td>\n      <td>90</td>\n      <td>236</td>\n      <td>241</td>\n      <td>130</td>\n      <td>854</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>38499</th>\n      <td>2384</td>\n      <td>170</td>\n      <td>15</td>\n      <td>60</td>\n      <td>5</td>\n      <td>90</td>\n      <td>230</td>\n      <td>245</td>\n      <td>143</td>\n      <td>864</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>38500</th>\n      <td>2383</td>\n      <td>165</td>\n      <td>13</td>\n      <td>60</td>\n      <td>4</td>\n      <td>67</td>\n      <td>231</td>\n      <td>244</td>\n      <td>141</td>\n      <td>875</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>38501 rows × 53 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, we have over 38,000 rows, each with 52 feature columns and 1 target column:\n\n* `Elevation`: Elevation in meters\n* `Aspect`: Aspect in degrees azimuth\n* `Slope`: Slope in degrees\n* `Horizontal_Distance_To_Hydrology`: Horizontal dist to nearest surface water features in meters\n* `Vertical_Distance_To_Hydrology`: Vertical dist to nearest surface water features in meters\n* `Horizontal_Distance_To_Roadways`: Horizontal dist to nearest roadway in meters\n* `Hillshade_9am`: Hillshade index at 9am, summer solstice\n* `Hillshade_Noon`: Hillshade index at noon, summer solstice\n* `Hillshade_3pm`: Hillshade index at 3pm, summer solstice\n* `Horizontal_Distance_To_Fire_Points`: Horizontal dist to nearest wildfire ignition points, meters\n* `Wilderness_Area_x`: Wilderness area designation (3 columns)\n* `Soil_Type_x`: Soil Type designation (39 columns)\n* `Cover_Type`: 1 for cottonwood/willow, 0 for ponderosa pine"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is also an imbalanced dataset, since cottonwood/willow trees are relatively rare in this forest:"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nprint(\"Raw Counts\")\nprint(df[\"Cover_Type\"].value_counts())\nprint()\nprint(\"Percentages\")\nprint(df[\"Cover_Type\"].value_counts(normalize=True))",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Raw Counts\n0    35754\n1     2747\nName: Cover_Type, dtype: int64\n\nPercentages\n0    0.928651\n1    0.071349\nName: Cover_Type, dtype: float64\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we had a model that *always* said that the cover type was ponderosa pine (class 0), what accuracy score would we get?"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate text\n\"\"\"\n~92.9%\n\"\"\"",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 7,
     "data": {
      "text/plain": "'\\n~92.9%\\n'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You will need to take this into account when working through this problem."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Requirements\n\n#### 1. Perform a Train-Test Split\n\nFor a complete end-to-end ML process, we need to create a holdout set that we will use at the very end to evaluate our final model's performance.\n\n#### 2. Build and Evaluate a Baseline Model\n\nWithout performing any preprocessing or hyperparameter tuning, build and evaluate a vanilla logistic regression model using log loss and `cross_val_score`.\n\n#### 3. Write a Custom Cross Validation Function\n\nBecause we are using preprocessing techniques that differ for train and validation data, we will need a custom function rather than simply preprocessing the entire `X_train` and using `cross_val_score` from scikit-learn.\n\n#### 4. Build and Evaluate Additional Logistic Regression Models\n\nUsing the function created in the previous step, build multiple logistic regression models with different hyperparameters in order to minimize log loss.\n\n#### 5. Choose and Evaluate a Final Model\n\nPreprocess the full training set and test set appropriately, then evaluate the final model with various classification metrics in addition to log loss."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Perform a Train-Test Split\n\nThis process should be fairly familiar by now. In the cell below, use the variable `df` (that has already been created) in order to create `X` and `y`, then training and test sets using `train_test_split` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n\nWe'll use a random state of 42 and `stratify=y` (to ensure an even balance of tree types) in the train-test split. Recall that the target is `Cover_Type`."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate code\n\n# Import the relevant function\nfrom sklearn.model_selection import train_test_split\n\n# Split df into X and y\nX = df.drop('Cover_Type', axis=1)\ny = df['Cover_Type']\n\n# Perform train-test split with random_state=42 and stratify=y\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that you have the correct data shape before proceeding:"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\n\n# X and y training data should have the same number of rows\nassert X_train.shape[0] == y_train.shape[0] and X_train.shape[0] == 28875\n\n# X and y testing data should have the same number of rows\nassert X_test.shape[0] == y_test.shape[0] and X_test.shape[0] == 9626\n\n# Both X should have 52 columns\nassert X_train.shape[1] == X_test.shape[1] and X_train.shape[1] == 52\n\n# Both y should have 1 column\nassert len(y_train.shape) == len(y_test.shape) and len(y_train.shape) == 1",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Also, we should have roughly equal percentages of cottonwood/willow trees for train vs. test targets:"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nprint(\"Train percent cottonwood/willow:\", y_train.value_counts(normalize=True)[1])\nprint(\"Test percent cottonwood/willow: \", y_test.value_counts(normalize=True)[1])",
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Train percent cottonwood/willow: 0.07134199134199135\nTest percent cottonwood/willow:  0.0713692083939331\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Build and Evaluate a Baseline Model\n\nUsing scikit-learn's `LogisticRegression` model, instantiate a classifier with `random_state=42`. Then use `cross_val_score` with `scoring=\"neg_log_loss\"` to find the average cross-validated log loss for this model on `X_train` and `y_train`.\n\n* [`LogisticRegression` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n* [`cross_val_score` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n\n(Similar to RMSE, the internal implementation of `cross_val_score` requires that we use \"negative log loss\" instead of just log loss. The code provided negates the result for you.)\n\n**The code below should produce a warning** but not an error. Because we have not scaled the data, we expect to get a `ConvergenceWarning` five times (once for each fold of cross validation)."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate code\n\n# Import relevant class and function\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate a LogisticRegression with random_state=42\nbaseline_model = LogisticRegression(random_state=42)\n\n# Use cross_val_score with scoring=\"neg_log_loss\" to evaluate the model\n# on X_train and y_train\nbaseline_neg_log_loss_cv = cross_val_score(baseline_model, X_train, y_train, scoring='neg_log_loss')\n\nbaseline_log_loss = -(baseline_neg_log_loss_cv.mean())\nbaseline_log_loss",
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "text": "/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n",
     "name": "stderr"
    },
    {
     "output_type": "execute_result",
     "execution_count": 11,
     "data": {
      "text/plain": "0.17221056770404072"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ok, so we are getting the `ConvergenceWarning`s we expected, and log loss of around 0.172 with our baseline model.\n\nIs that a \"good\" log loss? That's hard to say — log loss is not particularly interpretable. \n\nIf we had a model that just chose 0 (the majority class) every time, this is the log loss we would get:"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nfrom sklearn.metrics import log_loss\nimport numpy as np\n\nlog_loss(y_train, np.zeros(len(y_train)))",
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 12,
     "data": {
      "text/plain": "2.571426008020133"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loss is a metric where lower is better, so our baseline model is clearly an improvement over just guessing the majority class every time.\n\nEven though it is difficult to interpret, the 0.172 value will be a useful baseline as we continue modeling, to see if we are actually making improvements or just getting slightly better performance by chance.\n\nWe will also use other metrics at the last step in order to describe the final model's performance in a more user-friendly way."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Write a Custom Cross Validation Function\n\n### Conceptual Considerations\n\nFirst, consider: which preprocessing steps should be taken with this dataset? Recall that our data is imbalanced, and that it caused a `ConvergenceWarning` for our baseline model."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate text\n\"\"\"\nThere's an obvious class imbalance that we'll need to deal with\n\"\"\"",
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 13,
     "data": {
      "text/plain": "\"\\nThere's an obvious class imbalance that we'll need to deal with\\n\""
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you likely noted above, we should use some kind of resampling technique to address the large class imbalance. Let's use `SMOTE` (synthetic minority oversampling, [documentation here](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)), which creates synthetic examples of the minority class to help train the model.\n\nDoes SMOTE work just like a typical scikit-learn transformer, where you fit the transformer on the training data then transform both the training and the test data?"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate text\n\"\"\"\nNo SMOTE is only applied to the training data in order to generate samples for the minority class. Applying it also to the test data would cause data leakage.\n\"\"\"",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 14,
     "data": {
      "text/plain": "'\\nNo SMOTE is only applied to the training data in order to generate samples for the minority class. Applying it also to the test data would cause data leakage.\\n'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# __SOLUTION\n\"\"\"\nNo, SMOTE does not work like that. We never want to oversample the\nminority class in the test data, because then we are generating\nmetrics based on synthetic data and not actual data.\n\nInstead, we only want to fit and transform the training data, and\nleave the testing data alone.\n\"\"\"",
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 15,
     "data": {
      "text/plain": "'\\nNo, SMOTE does not work like that. We never want to oversample the\\nminority class in the test data, because then we are generating\\nmetrics based on synthetic data and not actual data.\\n\\nInstead, we only want to fit and transform the training data, and\\nleave the testing data alone.\\n'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you also likely noted above, we should use some transformer to normalize the data. Let's use a `StandardScaler` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)).\n\nDoes `StandardScaler` work just like a typical scikit-learn transformer, where you fit the transformer on the training data then transform both the training and the test data?"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate text\n\"\"\"\nYes it does\n\"\"\"",
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 16,
     "data": {
      "text/plain": "'\\nYes it does\\n'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "(At this point it's a good idea to double-check your answers against the `solution` branch to make sure you understand the setup.)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using `StratifiedKFold`\n\nAs you can see from the `cross_val_score` documentation linked above, \"under the hood\" it is using `StratifiedKFold` for classification tasks.\n\nEssentially `StratifiedKFold` is just providing the information you need to make 5 separate train-test splits inside of `X_train`. Then there is other logic within `cross_val_score` to fit and evaluate the provided model.\n\nSo, if our original code looked like this:\n\n```python\nbaseline_model = LogisticRegression(random_state=42)\nbaseline_neg_log_loss_cv = cross_val_score(baseline_model, X_train, y_train, scoring=\"neg_log_loss\")\nbaseline_log_loss = -(baseline_neg_log_loss_cv.mean())\nbaseline_log_loss\n```\n\nThe equivalent of the above code using `StratifiedKFold` would look something like this:"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone\n\n# Negative log loss doesn't exist as something we can import,\n# but we can create it\nneg_log_loss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n\n# Instantiate the model (same as previous example)\nbaseline_model = LogisticRegression(random_state=42)\n\n# Create a list to hold the score from each fold\nkfold_scores = np.ndarray(5)\n\n# Instantiate a splitter object and loop over its result\nkfold = StratifiedKFold()\nfor fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n    # Extract train and validation subsets using the provided indices\n    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n    \n    # Clone the provided model and fit it on the train subset\n    temp_model = clone(baseline_model)\n    temp_model.fit(X_t, y_t)\n    \n    # Evaluate the provided model on the validation subset\n    neg_log_loss_score = neg_log_loss(temp_model, X_val, y_val)\n    kfold_scores[fold] = neg_log_loss_score\n    \n-(kfold_scores.mean())",
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "text": "/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n",
     "name": "stderr"
    },
    {
     "output_type": "execute_result",
     "execution_count": 17,
     "data": {
      "text/plain": "0.17221056770404072"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, this produced the same result as our original cross validation (including the `ConvergenceWarning`s):"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nprint(baseline_neg_log_loss_cv)\nprint(kfold_scores)",
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "text": "[-0.1716005  -0.17457811 -0.16303282 -0.17948572 -0.17235568]\n[-0.1716005  -0.17457811 -0.16303282 -0.17948572 -0.17235568]\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So, what is the point of doing it this way, instead of the much-shorter `cross_val_score` approach?\n\n**Using `StratifiedKFold` \"by hand\" allows us to customize what happens inside of that loop.**\n\nTherefore we can apply these preprocessing techniques appropriately:\n\n1. Fit a `StandardScaler` object on the training subset (not the full training data) and transform both the train and test subsets\n2. Fit a `SMOTE` object and transform only the training subset"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Writing a Custom Cross Validation Function with `StratifiedKFold`\n\nIn the cell below, we have set up a function `custom_cross_val_score` that has an interface that resembles the `cross_val_score` function from scikit-learn.\n\nMost of it is set up for you already, all you need to do is add the `SMOTE` and `StandardScaler` steps described above."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate code\n\n# Import relevant sklearn and imblearn classes\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\n\ndef custom_cross_val_score(estimator, X, y):\n    # Create a list to hold the scores from each fold\n    kfold_train_scores = np.ndarray(5)\n    kfold_val_scores = np.ndarray(5)\n\n    # Instantiate a splitter object and loop over its result\n    kfold = StratifiedKFold(n_splits=5)\n    for fold, (train_index, val_index) in enumerate(kfold.split(X, y)):\n        # Extract train and validation subsets using the provided indices\n        X_t, X_val = X.iloc[train_index], X.iloc[val_index]\n        y_t, y_val = y.iloc[train_index], y.iloc[val_index]\n        \n        # Instantiate StandardScaler\n        scaler = StandardScaler()\n        # Fit and transform X_t\n        X_t_scaled = scaler.fit_transform(X_t)\n        # Transform X_val\n        X_val_scaled = scaler.transform(X_val)\n        \n        # Instantiate SMOTE with random_state=42 and sampling_strategy=0.28\n        sm = SMOTE(random_state=42, sampling_strategy=0.28)\n        # Fit and transform X_t_scaled and y_t using sm\n        X_t_oversampled, y_t_oversampled = sm.fit_resample(X_t_scaled, y_t)\n        \n        # Clone the provided model and fit it on the train subset\n        temp_model = clone(estimator)\n        temp_model.fit(X_t_oversampled, y_t_oversampled)\n        \n        # Evaluate the provided model on the train and validation subsets\n        neg_log_loss_score_train = neg_log_loss(temp_model, X_t_oversampled, y_t_oversampled)\n        neg_log_loss_score_val = neg_log_loss(temp_model, X_val_scaled, y_val)\n        kfold_train_scores[fold] = neg_log_loss_score_train\n        kfold_val_scores[fold] = neg_log_loss_score_val\n        \n    return kfold_train_scores, kfold_val_scores\n        \nmodel_with_preprocessing = LogisticRegression(random_state=42, class_weight={1: 0.28})\npreprocessed_train_scores, preprocessed_neg_log_loss_cv = custom_cross_val_score(model_with_preprocessing, X_train, y_train)\n- (preprocessed_neg_log_loss_cv.mean())",
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 19,
     "data": {
      "text/plain": "0.13235899550278746"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The output you get should be about 0.132, and there should no longer be a `ConvergenceWarning`.\n\nIf you're not getting the right output, double check that you are applying the correct transformations to the correct variables:\n\n1. `X_t` should be scaled to create `X_t_scaled`, then `X_t_scaled` should be resampled to create `X_t_oversampled`, then `X_t_oversampled` should be used to fit the model\n2. `X_val` should be scaled to create `X_val_scaled`, then `X_val_scaled` should be used to evaluate `neg_log_loss`\n3. `y_t` should be resampled to create `y_t_oversampled`, then `y_t_oversampled` should be used to fit the model\n4. `y_val` should not be transformed in any way. It should just be used to evaluate `neg_log_loss`\n\nAnother thing to check is that you used `sampling_strategy=0.28` when you instantiated the `SMOTE` object."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you are getting the right output, great!  Let's compare that to our baseline log loss:"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nprint(-baseline_neg_log_loss_cv.mean())\nprint(-preprocessed_neg_log_loss_cv.mean())",
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "text": "0.17221056770404072\n0.13235899550278746\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Looks like our preprocessing with `StandardScaler` and `SMOTE` has provided some improvement over the baseline! Let's move on to Step 4."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Build and Evaluate Additional Logistic Regression Models\n\nNow that we have applied appropriate preprocessing steps to our data in our custom cross validation function, we can reuse that function to test multiple different `LogisticRegression` models.\n\nFor each model iteration, make sure you specify `class_weight={1: 0.28}`, because this aligns with the weighting created by our `SMOTE` process.\n\n### Where to Next?\n\nOne of the first questions to ask when you start iterating on any model is: ***are we overfitting***? Many of the models you will learn during this course will have built-in functionality to reduce overfitting.\n\nTo determine whether we are overfitting, let's examine the training scores vs. the validation scores from our existing modeling process:"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nprint(\"Train:     \", -preprocessed_train_scores)\nprint(\"Validation:\", -preprocessed_neg_log_loss_cv)",
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Train:      [0.29227141 0.28801243 0.29282026 0.28652204 0.28910185]\nValidation: [0.13067576 0.13636961 0.12628191 0.13715658 0.13131112]\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remember that these are loss metrics, meaning lower is better. Are we overfitting?"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate text\n\"\"\"\nNo, it's possible we are underfitting, perhaps because the regularization is too high\n\"\"\"",
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 22,
     "data": {
      "text/plain": "\"\\nNo, it's possible we are underfitting, perhaps because the regularization is too high\\n\""
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It's actually possible that we are underfitting due to too high of regularization. Remember that `LogisticRegression` from scikit-learn has regularization by default"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nmodel_with_preprocessing.get_params()",
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 23,
     "data": {
      "text/plain": "{'C': 1.0,\n 'class_weight': {1: 0.28},\n 'dual': False,\n 'fit_intercept': True,\n 'intercept_scaling': 1,\n 'l1_ratio': None,\n 'max_iter': 100,\n 'multi_class': 'auto',\n 'n_jobs': None,\n 'penalty': 'l2',\n 'random_state': 42,\n 'solver': 'lbfgs',\n 'tol': 0.0001,\n 'verbose': 0,\n 'warm_start': False}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That first key-value pair, `'C': 1.0`, specifies the regularization strength. As is noted in the [scikit-learn `LogisticRegression` docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), `C` is:\n\n> Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n\nIn general if you are increasing `C` you want to increase it by orders of magnitude. I.e. not increasing it to 1.1, but rather increasing it to 1e3, 1e5, etc."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reducing Regularization\n\nIn the cell below, instantiate a `LogisticRegression` model with lower regularization (i.e. higher `C`), along with `random_state=42` and `class_weight={1: 0.28}`. Call this model `model_less_regularization`."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate code\n\nmodel_less_regularization = LogisticRegression(C=1e3, random_state=42, class_weight={1: 0.28})",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code cell double-checks that the model was created correctly:"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\n\n# Check variable type\nassert type(model_less_regularization) == LogisticRegression\n\n# Check params\nassert model_less_regularization.get_params()[\"random_state\"] == 42\nassert model_less_regularization.get_params()[\"class_weight\"] == {1: 0.28}\nassert model_less_regularization.get_params()[\"C\"] != 1.0",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, evaluate that model using `custom_cross_val_score`. Recall that `custom_cross_val_score` takes 3 arguments: `estimator`, `X`, and `y`. In this case, `estimator` should be `model_less_regularization`, `X` should be `X_train`, and `y` should be `y_train`."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate code\nless_regularization_train_scores, less_regularization_val_scores = custom_cross_val_score(estimator=model_less_regularization, X=X_train, y=y_train)\n\nprint(\"Previous Model\")\nprint(\"Train average:     \", -preprocessed_train_scores.mean())\nprint(\"Validation average:\", -preprocessed_neg_log_loss_cv.mean())\nprint(\"Current Model\")\nprint(\"Train average:     \", -less_regularization_train_scores.mean())\nprint(\"Validation average:\", -less_regularization_val_scores.mean())",
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Previous Model\nTrain average:      0.28974560104038344\nValidation average: 0.13235899550278746\nCurrent Model\nTrain average:      0.2895735081461482\nValidation average: 0.13234430596963265\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Your answers will vary somewhat depending on the value of `C` that you chose, but in general you should see a slight improvement, from something like 0.132358 validation average to 0.132344 (improvement of .000014). Not a massive difference but it is an improvement!"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Alternative Solver\n\nRight now we are using the default solver and type of regularization penalty:"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nprint(\"solver:\", model_less_regularization.get_params()[\"solver\"])\nprint(\"penalty:\", model_less_regularization.get_params()[\"penalty\"])",
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "text": "solver: lbfgs\npenalty: l2\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What if we want to try a different kind of regularization penalty?\n\nFrom the docs:\n\n> * ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n> * ‘liblinear’ and ‘saga’ also handle L1 penalty\n> * ‘saga’ also supports ‘elasticnet’ penalty\n\nIn other words, the only models that support L1 or elastic net penalties are `liblinear` and `saga`. `liblinear` is going to be quite slow with the size of our dataset, so let's use `saga`.\n\nIn the cell below, create a model that uses `solver=\"saga\"` and `penalty=\"elasticnet\"`. Then use the `l1_ratio` argument to specify the mixing of L1 and L2 regularization. You want a value greater than zero (zero would just mean using L2 regularization) and less than one (one would mean using just L1 regularization).\n\nRemember to also specify `random_state=42`, `class_weight={1: 0.28}`, and `C` equals the value you previously used."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate code\nmodel_alternative_solver = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.5, random_state=42, class_weight={1: 0.28})\n\nalternative_solver_train_scores, alternative_solver_val_scores = custom_cross_val_score(\n    model_alternative_solver,\n    X_train,\n    y_train\n)\n\nprint(\"Previous Model (Less Regularization)\")\nprint(\"Train average:     \", -less_regularization_train_scores.mean())\nprint(\"Validation average:\", -less_regularization_val_scores.mean())\nprint(\"Current Model\")\nprint(\"Train average:     \", -alternative_solver_train_scores.mean())\nprint(\"Validation average:\", -alternative_solver_val_scores.mean())",
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "text": "/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n",
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": "Previous Model (Less Regularization)\nTrain average:      0.2895735081461482\nValidation average: 0.13234430596963265\nCurrent Model\nTrain average:      0.29312129872369674\nValidation average: 0.13608329056449103\n",
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": "/private/var/containers/Bundle/Application/3980AAF1-1B42-4BA8-9D03-CB4DA93BADE1/Juno.app/pinned-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n",
     "name": "stderr"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Most likely you started getting `ConvergenceWarning`s again, even though we are scaling the data inside of `custom_cross_val_score`. When you get a convergence warning in a case like this, you want to modify the `tol` and/or `max_iter` parameters."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adjusting Gradient Descent Parameters\n\nIf you are getting good results (good metrics) but are still getting a `ConvergenceWarning`, consider increasing the tolerance (`tol` argument). The tolerance specifies how close to zero the gradient must be in order to stop taking additional steps. It's possible that your model is finding a gradient that is close enough to zero, but slightly above the default tolerance, if everything otherwise looks good.\n\nIn this case, we are getting slightly worse metrics on both the train and the validation data (compared to a different solver strategy), so increasing the number of iterations (`max_iter`) seems like a better strategy. Essentially this is allowing the gradient descent algorithm to take more steps as it tries to find an optimal solution.\n\nIn the cell below, create a model called `model_more_iterations` that has the same hyperparameters as `model_alternative_solver`, with the addition of an increased `max_iter`. You'll need to increase `max_iter` significantly to a number in the thousands.\n\n**Note:** As you increase `max_iter`, it is normal for the execution time of fitting the model to increase. The following cell may take up to several minutes to execute. Try to be patient with this exercise! If this step times out, you can just read on ahead."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate code\nmodel_more_iterations = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.5, random_state=42, class_weight={1: 0.28}, max_iter=10000)\n\n\nmore_iterations_train_scores, more_iterations_val_scores = custom_cross_val_score(\n    model_more_iterations,\n    X_train,\n    y_train\n)\n\nprint(\"Previous Best Model (Less Regularization)\")\nprint(\"Train average:     \", -less_regularization_train_scores.mean())\nprint(\"Validation average:\", -less_regularization_val_scores.mean())\nprint(\"Previous Model with This Solver\")\nprint(\"Train average:     \", -alternative_solver_train_scores.mean())\nprint(\"Validation average:\", -alternative_solver_val_scores.mean())\nprint(\"Current Model\")\nprint(\"Train average:     \", -more_iterations_train_scores.mean())\nprint(\"Validation average:\", -more_iterations_val_scores.mean())",
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Previous Best Model (Less Regularization)\nTrain average:      0.2895735081461482\nValidation average: 0.13234430596963265\nPrevious Model with This Solver\nTrain average:      0.29312129872369674\nValidation average: 0.13608329056449103\nCurrent Model\nTrain average:      0.2898272447962492\nValidation average: 0.13244137752421734\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The results you got are most likely around 0.13241, whereas the previous model was around 0.13234. In other words, even after waiting all that time, we are getting 0.00007 worse log loss with this solver.\n\nThis is a fairly typical experience when hyperparameter tuning! Often the default hyperparameters are the default because they work best in the most situations. This is especially true of logistic regression, which has relatively few hyperparameters. Once we get to more complex models, there are more \"levers to pull\" (hyperparameters to adjust) so it is more likely that we'll improve performance by deviating from the default.\n\nLet's declare the `model_less_regularization` to be our best model, and move on to the final evaluation phase."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Choose and Evaluate a Final Model"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nfinal_model = model_less_regularization",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In order to evaluate our final model, we need to preprocess the full training and test data, fit the model on the full training data, and evaluate it on the full test data. Initially we'll continue to use log loss for the evaluation step.\n\n### Preprocessing the Full Dataset"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate code\n\n# Instantiate StandardScaler\nscaler = StandardScaler()\n# Fit and transform X_train\nX_train_scaled = scaler.fit_transform(X_train)\n# Transform X_test\nX_test_scaled = scaler.transform(X_test)\n\n# Instantiate SMOTE with random_state=42 and sampling_strategy=0.28\nsm = SMOTE(random_state=42, sampling_strategy=0.28)\n# Fit and transform X_train_scaled and y_train using sm\nX_train_oversampled, y_train_oversampled = sm.fit_resample(X_train_scaled, y_train)",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fitting the Model on the Full Training Data"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nfinal_model.fit(X_train_oversampled, y_train_oversampled)",
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 32,
     "data": {
      "text/plain": "LogisticRegression(C=1000.0, class_weight={1: 0.28}, random_state=42)",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1000.0, class_weight={1: 0.28}, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1000.0, class_weight={1: 0.28}, random_state=42)</pre></div></div></div></div></div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluating the Model on the Test Data\n\n#### Log Loss"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nlog_loss(y_test, final_model.predict_proba(X_test_scaled))",
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 33,
     "data": {
      "text/plain": "0.13031141321139694"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Great! We are getting slightly better performance when we train the model with the full training set, compared to the average cross-validated performance. This is typical since models tend to perform better with more training data.\n\nThis model has improved log loss compared to our initial baseline model, which had about 0.172.\n\nBut we're not quite done here!\n\nIf we wanted to present this forest cover classification model to non-technical stakeholders, log loss would be a confusing choice. Let's compute some other metrics that tell the story of our model's performance in a more interpretable way."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Accuracy\n\nAlthough we noted the issues with accuracy as a metric on unbalanced datasets, accuracy is a very intuitive metric. Recall that we would expect an accuracy of about 0.928651 if we identified every cell as class 0. What accuracy do we get with our new model?\n\n(Note that we used `.predict_proba` above, but accuracy uses `.predict`)"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate code\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, final_model.predict(X_test_scaled))",
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 34,
     "data": {
      "text/plain": "0.9456679825472678"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In other words, our model correctly identifies the type of forest cover about 94.6% of the time, whereas always guessing the majority class (ponderosa pine) would only be accurate about 92.9% of the time."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Precision\n\nIf we always chose the majority class, we would expect a precision of 0, since we would never identify any \"true positives\". What is the precision of our final model? Use `precision_score` from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html))."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate code\n\n# Import the relevant function\nfrom sklearn.metrics import precision_score\n\n# Display the precision score\nprecision_score(y_test, final_model.predict(X_test_scaled))",
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 35,
     "data": {
      "text/plain": "0.6659919028340081"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In other words, if our model labels a given cell of forest as class 1, there is about a 66.6% chance that it is actually class 1 (cottonwood/willow) and about a 33.4% chance that it is actually class 0 (ponderosa pine)."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Recall\n\nAgain, if we always chose the majority class, we would expect a recall of 0. What is the recall of our final model? Use `recall_score` from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html))."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Replace None with appropriate code\n\n# Import the relevant function\nfrom sklearn.metrics import recall_score\n\n# Display the recall score\nrecall_score(y_test, final_model.predict(X_test_scaled))",
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 36,
     "data": {
      "text/plain": "0.47889374090247455"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In other words, if a given cell of forest is actually class 1, there is about a 47.9% chance that our model will correctly label it as class 1 (cottonwood/willow) and about a 52.1% chance that our model will incorrectly label it as class 0 (ponderosa pine)."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Interpretation\n\nDepending on the stakeholder, you most likely want to report just precision or just recall. Try to understand their business case:\n\n* If false positives are a bigger problem (labeled cottonwood/willow when it's really ponderosa pine), precision is the important metric to report\n* If false negatives are a bigger problem (labeled ponderosa pine when it's really cottonwood/willow), recall is the important metric to report\n\nIf those problems have truly equal importance, you could report an f1-score instead, although this is somewhat more difficult for the average person to interpret.\n\n#### BONUS: Adjusting the Decision Threshold\n\nIf either of those problems is important enough that it outweighs overall accuracy, you could also adjust the decision threshold of your final model to improve the metric that matters most. Let's say that it's important to improve the recall score — that we want to be able to correctly label more of the cottonwood/willow trees as cottonwood/willow trees, even if that means accidentally labeling more ponderosa pine as cottonwood/willow incorrectly.\n\nThen we can use `.predict_proba` to err on the side of the positive class. Let's use an exaggerated example, which assumes that false negatives are a very significant problem. Instead of using the default 50% threshold (where a probability over 0.5 is classified as positive) let's say that if there is greater than a 1% chance it's positive, we classify it as positive:\n\n(If the opposite issue were the case — it's very important that every area classified as 1 is actually cottonwood/willow — you would want the threshold to be higher than 50% rather than lower than 50%.)"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\n\ndef final_model_func(model, X):\n    \"\"\"\n    Custom function to predict probability of\n    cottonwood/willow. If the model says there\n    is >1% chance, we label it as class 1\n    \"\"\"\n    probs = model.predict_proba(X)[:,1]\n    return [int(prob > 0.01) for prob in probs]\n\n# Calculate predictions with adjusted threshold and\n# display proportions of predictions\nthreshold_adjusted_probs = pd.Series(final_model_func(final_model, X_test_scaled))\nthreshold_adjusted_probs.value_counts(normalize=True)",
   "execution_count": 38,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 38,
     "data": {
      "text/plain": "0    0.586433\n1    0.413567\ndtype: float64"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So, now we are predicting that everything over a 1% chance of being class 1 as class 1, which means that we're classifying about 58.6% of the records as class 0 and 41.4% of the records as class 1."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Run this cell without changes\nprint(\"Accuracy:\", accuracy_score(y_test, threshold_adjusted_probs))\nprint(\"Recall:  \", recall_score(y_test, threshold_adjusted_probs))",
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Accuracy: 0.6565551630999377\nRecall:   0.9912663755458515\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This means that we are able to identify 99.1% of the true positives (i.e. 99.1% of the cottonwood/willow cells are identified). However this comes at a cost; our overall accuracy is now 65.7% instead of over 90%.\n\nSo we are classifying over 40% of the cells as cottonwood/willow, even though fewer than 10% of the cells are actually that category, in order to miss as few true positives as possible. Even though this seems fairly extreme, our model is still better than just choosing class 1 every time (that model would have about 7% accuracy).\n\nThis kind of model might be useful if there is some kind of treatment needed for cottonwood/willow trees, but your organization only has the resources to visit fewer than half of the study areas. This model would allow them to visit 41% of the areas and successfully treat over 99% of the cottonwood/willow trees.\n\nYou can also imagine a less-drastic version of this threshold adjusting, where you trade off a marginal improvement in precision or recall for a marginal reduction in accuracy. Visually inspecting the precision-recall curve ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_precision_recall_curve.html)) can help you choose the threshold based on what you want to optimize."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Summary\n\nIn this lab, you completed an end-to-end machine learning modeling process with logistic regression on an imbalanced dataset. First you built and evaluated a baseline model. Next you wrote a custom cross validation function in order to use SMOTE resampling appropriately (without needing an `imblearn` pipeline). After that, you tuned the model through adjusting the regularization strength and the gradient descent hyperparameters. Finally, you evaluated your final model on log loss as well as more user-friendly metrics."
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}